{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video Game Reviews Text Analysis\n",
    "\n",
    "By Fei Guo, Victor Li, Jonathan Lin, Xinyu Zheng\n",
    "\n",
    "INFX 575 Calulate Jargon Distance of similar authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read from results.csv file\n",
    "import csv\n",
    "        \n",
    "reader = csv.reader(open(\"results.csv\", 'rU'), dialect='excel')\n",
    "\n",
    "dates = []\n",
    "games = []\n",
    "links = []\n",
    "platforms = []\n",
    "reviews = []\n",
    "writers = []\n",
    "scores = []\n",
    "scores_100 = []\n",
    "sites = []\n",
    "\n",
    "for col in reader:\n",
    "    dates.append(col[0])\n",
    "    games.append(col[1])\n",
    "    links.append(col[2])\n",
    "    platforms.append(col[3])\n",
    "    reviews.append(col[4])\n",
    "    writers.append(col[5])\n",
    "    scores.append(col[6])\n",
    "    scores_100.append(col[7])\n",
    "    sites.append(col[8])\n",
    "    \n",
    "dates = dates[1:]\n",
    "games = games[1:]\n",
    "links = links[1:]\n",
    "platforms = platforms[1:]\n",
    "reviews = reviews[1:]\n",
    "writers = writers[1:]\n",
    "scores = scores[1:]\n",
    "scores_100 = scores_100[1:]\n",
    "sites = sites[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dates</th>\n",
       "      <th>games</th>\n",
       "      <th>links</th>\n",
       "      <th>platforms</th>\n",
       "      <th>reviews</th>\n",
       "      <th>writers</th>\n",
       "      <th>scores</th>\n",
       "      <th>scores_100</th>\n",
       "      <th>sites</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 2008-05-23 13:00:00</td>\n",
       "      <td> SingStar</td>\n",
       "      <td> http://www.giantbomb.com/api/review/1900-1/</td>\n",
       "      <td> PlayStation 3</td>\n",
       "      <td> All these karaoke video games have got it all ...</td>\n",
       "      <td> Jeff Gerstmann</td>\n",
       "      <td> 4.0</td>\n",
       "      <td> 80.0</td>\n",
       "      <td> Giant Bomb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 2008-05-20 17:00:00</td>\n",
       "      <td>     Haze</td>\n",
       "      <td> http://www.giantbomb.com/api/review/1900-2/</td>\n",
       "      <td> PlayStation 3</td>\n",
       "      <td> Haze attempts to deal with the concept of wart...</td>\n",
       "      <td> Jeff Gerstmann</td>\n",
       "      <td> 2.0</td>\n",
       "      <td> 40.0</td>\n",
       "      <td> Giant Bomb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 dates     games                                        links  \\\n",
       "0  2008-05-23 13:00:00  SingStar  http://www.giantbomb.com/api/review/1900-1/   \n",
       "1  2008-05-20 17:00:00      Haze  http://www.giantbomb.com/api/review/1900-2/   \n",
       "\n",
       "       platforms                                            reviews  \\\n",
       "0  PlayStation 3  All these karaoke video games have got it all ...   \n",
       "1  PlayStation 3  Haze attempts to deal with the concept of wart...   \n",
       "\n",
       "          writers scores scores_100       sites  \n",
       "0  Jeff Gerstmann    4.0       80.0  Giant Bomb  \n",
       "1  Jeff Gerstmann    2.0       40.0  Giant Bomb  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save to a data frame\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "index = range(len(games))\n",
    "columns = ['dates','games','links','platforms','reviews','writers','scores','scores_100','sites']\n",
    "\n",
    "review_data = pd.DataFrame(index=index, columns=columns)\n",
    "review_data['dates'] = dates\n",
    "review_data['games'] = games\n",
    "review_data['links'] = links\n",
    "review_data['platforms'] = platforms\n",
    "review_data['reviews'] = reviews\n",
    "review_data['writers'] = writers\n",
    "review_data['scores'] = scores\n",
    "review_data['scores_100'] = scores_100\n",
    "review_data['sites'] = sites\n",
    "\n",
    "review_data.ix[:1, :9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a new data frame and populate with writers, combined reviews of each writer, and average scores of each writer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "index = range(len(writers))\n",
    "columns = ['writers','scores','reviews']\n",
    "\n",
    "author_review = pd.DataFrame(index=index, columns=columns)\n",
    "author_review['writers'] = writers\n",
    "author_review['scores'] = scores_100\n",
    "author_review['scores'] = author_review['scores'].convert_objects(convert_numeric=True)\n",
    "author_review['reviews'] = reviews\n",
    "\n",
    "# print author_review.ix[:1, :3]\n",
    "# print author_review.shape\n",
    "\n",
    "author_review['reviews'] = author_review[['writers','scores','reviews']].groupby(['writers'])['reviews'].transform(lambda x: ','.join(x))\n",
    "# author_review = author_review[['writers','scores','reviews']].drop_duplicates()\n",
    "author_review = author_review.reset_index()\n",
    "# print author_review\n",
    "\n",
    "author_avg_scores = author_review.groupby(['writers','reviews'], as_index=False)\n",
    "author_avg_scores = author_avg_scores['scores'].mean()\n",
    "author_avg_scores = pd.DataFrame(author_avg_scores)\n",
    "# print author_avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.7648648649\n"
     ]
    }
   ],
   "source": [
    "# calculate average score of all writers\n",
    "review_data['scores_100'] = review_data['scores_100'].convert_objects(convert_numeric=True)\n",
    "avg_score = float(sum(review_data['scores_100']))/len(review_data['scores_100'])\n",
    "print avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_scores = author_avg_scores.sort(['scores'], ascending=True)\n",
    "\n",
    "sorted_scores = sorted_scores.reset_index()\n",
    "sorted_scores\n",
    "\n",
    "groups_writers = [[]]*10\n",
    "groups_reviews = [[]]*10\n",
    "groups_scores = [[]]*10\n",
    "\n",
    "for k in range(10):\n",
    "    groups_writers[k] = []\n",
    "    groups_reviews[k] = []\n",
    "    groups_scores[k] = []\n",
    "\n",
    "for i in range(len(sorted_scores)):   \n",
    "    current_writer = sorted_scores.ix[i,'writers']\n",
    "    current_review = sorted_scores.ix[i,'reviews']\n",
    "    current_score = sorted_scores.ix[i,'scores']\n",
    "    \n",
    "    for j in range(10):       \n",
    "        if (current_score <= (j+1)*10) and (current_score > j*10):\n",
    "            groups_writers[j].append(current_writer)\n",
    "            groups_reviews[j].append(current_review)\n",
    "            groups_scores[j].append(current_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ignore this section\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = [[]]*10\n",
    "\n",
    "for i in range(10):\n",
    "    index = range(len(groups_writers[i]))\n",
    "    column = ['writers','reviews','scores']\n",
    "    df[i] = pd.DataFrame(index=index, columns=column)\n",
    "    df[i]['writers'] = groups_writers[i]\n",
    "    df[i]['reviews'] = groups_reviews[i]\n",
    "    df[i]['scores'] = groups_scores[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_words(token_words,index):\n",
    "    uni_cleaned = ''\n",
    "    vocab_tmp = []\n",
    "    stemmer=stem.PorterStemmer()\n",
    "    for w in token_words:\n",
    "        w = w.lower()\n",
    "        if w not in stopset:\n",
    "            tmp = stemmer.stem(w)\n",
    "            tmp = unicodedata.normalize('NFKD', tmp).encode('ascii','ignore')\n",
    "            if tmp not in string.punctuation:\n",
    "                if not tmp.isdigit():\n",
    "                    if tmp not in letters:\n",
    "                        if tmp not in special_char:                               \n",
    "                            vocab_tmp.append(tmp)\n",
    "    for u in vocab_tmp:\n",
    "        if (vocab_tmp.count(u) >= 5):\n",
    "            uni_cleaned = uni_cleaned + u + ' '\n",
    "            vocab[index].append(u)\n",
    "    return uni_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import unicodedata\n",
    "from nltk import stem\n",
    "import string\n",
    "\n",
    "stopset = set(stopwords.words('english'))\n",
    "letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "special_char = ['~~','``',\"''\", '--',\"'ll\",\"n't\",\"'re\",\"'s\",\"'d\"]\n",
    "\n",
    "vocab = [[]]*10\n",
    "for i in range(10):\n",
    "    vocab[i] = []\n",
    "\n",
    "for i in range(len(groups_writers[0])):\n",
    "    with open('/Users/XinyuZ/group_1/'+groups_writers[0][i]+'.txt', 'w') as f:\n",
    "        tokens=word_tokenize(str(groups_reviews[0][i]).decode(\"ISO-8859-1\"))\n",
    "        cleaned_review = clean_words(tokens,0)\n",
    "        f.write(cleaned_review)\n",
    "        f.close()\n",
    "\n",
    "for i in range(len(groups_writers[1])):\n",
    "    with open('/Users/XinyuZ/group_2/'+groups_writers[1][i]+'.txt', 'w') as f:\n",
    "        tokens=word_tokenize(str(groups_reviews[1][i]).decode(\"ISO-8859-1\"))\n",
    "        cleaned_review = clean_words(tokens,1)\n",
    "        f.write(cleaned_review)\n",
    "        f.close()\n",
    "\n",
    "for i in range(len(groups_writers[2])):\n",
    "    with open('/Users/XinyuZ/group_3/'+groups_writers[2][i]+'.txt', 'w') as f:\n",
    "        tokens=word_tokenize(str(groups_reviews[2][i]).decode(\"ISO-8859-1\"))\n",
    "        cleaned_review = clean_words(tokens,2)\n",
    "        f.write(cleaned_review)\n",
    "        f.close()\n",
    "\n",
    "for i in range(len(groups_writers[3])):\n",
    "    with open('/Users/XinyuZ/group_4/'+groups_writers[3][i]+'.txt', 'w') as f:\n",
    "        tokens=word_tokenize(str(groups_reviews[3][i]).decode(\"ISO-8859-1\"))\n",
    "        cleaned_review = clean_words(tokens,3)\n",
    "        f.write(cleaned_review)\n",
    "        f.close()\n",
    "        \n",
    "for i in range(len(groups_writers[4])):\n",
    "    with open('/Users/XinyuZ/group_5/'+groups_writers[4][i]+'.txt', 'w') as f:\n",
    "        tokens=word_tokenize(str(groups_reviews[4][i]).decode(\"ISO-8859-1\"))\n",
    "        cleaned_review = clean_words(tokens,4)\n",
    "        f.write(cleaned_review)\n",
    "        f.close()\n",
    "        \n",
    "for i in range(len(groups_writers[5])):\n",
    "    with open('/Users/XinyuZ/group_6/'+groups_writers[5][i]+'.txt', 'w') as f:\n",
    "        tokens=word_tokenize(str(groups_reviews[5][i]).decode(\"ISO-8859-1\"))\n",
    "        cleaned_review = clean_words(tokens,5)\n",
    "        f.write(cleaned_review)\n",
    "        f.close()\n",
    "        \n",
    "for i in range(len(groups_writers[6])):\n",
    "    with open('/Users/XinyuZ/group_7/'+groups_writers[6][i]+'.txt', 'w') as f:\n",
    "        tokens=word_tokenize(str(groups_reviews[6][i]).decode(\"ISO-8859-1\"))\n",
    "        cleaned_review = clean_words(tokens,6)\n",
    "        f.write(cleaned_review)\n",
    "        f.close()\n",
    "        \n",
    "for i in range(len(groups_writers[7])):\n",
    "    with open('/Users/XinyuZ/group_8/'+groups_writers[7][i]+'.txt', 'w') as f:\n",
    "        tokens=word_tokenize(str(groups_reviews[7][i]).decode(\"ISO-8859-1\"))\n",
    "        cleaned_review = clean_words(tokens,7)\n",
    "        f.write(cleaned_review)\n",
    "        f.close()\n",
    "        \n",
    "for i in range(len(groups_writers[8])):\n",
    "    with open('/Users/XinyuZ/group_9/'+groups_writers[8][i]+'.txt', 'w') as f:\n",
    "        tokens=word_tokenize(str(groups_reviews[8][i]).decode(\"ISO-8859-1\"))\n",
    "        cleaned_review = clean_words(tokens,8)\n",
    "        f.write(cleaned_review)\n",
    "        f.close()\n",
    "        \n",
    "for i in range(len(groups_writers[9])):\n",
    "    with open('/Users/XinyuZ/group_10/'+groups_writers[9][i]+'.txt', 'w') as f:\n",
    "        tokens=word_tokenize(str(groups_reviews[9][i]).decode(\"ISO-8859-1\"))\n",
    "        cleaned_review = clean_words(tokens,9)\n",
    "        f.write(cleaned_review)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group_vocab = [[]]*7\n",
    "\n",
    "group_vocab[0] = vocab[1]\n",
    "group_vocab[1] = vocab[4]\n",
    "group_vocab[2] = vocab[5]\n",
    "group_vocab[3] = vocab[6]\n",
    "group_vocab[4] = vocab[7]\n",
    "group_vocab[5] = vocab[8]\n",
    "group_vocab[6] = vocab[9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate corpus with 'cleaned' groups\n",
    "corpus_vocab = [[]]*7\n",
    "\n",
    "for i in range(0,6):\n",
    "    corpus_vocab[i] = [[]]*7\n",
    "    for j in range(i+1,7): \n",
    "        corpus_vocab[i][j] = []\n",
    "        for wi in group_vocab[i]:\n",
    "            corpus_vocab[i][j].append(wi)\n",
    "        for wj in group_vocab[j]:\n",
    "            corpus_vocab[i][j].append(wj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.42458642082\n",
      "6.26341431311\n",
      "9.36071008837\n",
      "10.3726120367\n",
      "10.1739253829\n",
      "8.94798903083\n",
      "6.43430583605\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# calculate H value for each group\n",
    "def get_H(group_word):\n",
    "    log_sum_group = 0\n",
    "    prob_count = Counter(group_word)\n",
    "    freq_sum = sum(prob_count.itervalues())\n",
    "    vocab = list(set(group_word))\n",
    "    for w in vocab:  \n",
    "        w_prob = prob_count[w]/float(freq_sum)\n",
    "        if (w_prob > 0):\n",
    "            log_sum_group += -w_prob * math.log(w_prob,2)\n",
    "    return log_sum_group\n",
    "\n",
    "H = [0]*7\n",
    "for i in range(0,7):\n",
    "    H[i] = get_H(group_vocab[i])\n",
    "    print H[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THIS SECTION OF CODE TOOK ABOUT 40 MINUTES TO FINISH RUNNING AND I CANNOT FIGURE OUT HOW TO OPTIMIZE\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# calculate Q values between groups\n",
    "def get_Qij(groupi_word,groupj_word,groupij_word):\n",
    "    tmp_Qij = 0\n",
    "    alpha = 0.01\n",
    "    \n",
    "    count = Counter(groupi_word)\n",
    "    freq = sum(count.itervalues())\n",
    "    vocab = list(set(groupi_word))\n",
    "    \n",
    "    count_ij = Counter(groupij_word)\n",
    "    freq_ij = sum(count_ij.itervalues())\n",
    "    \n",
    "    count_j = Counter(groupj_word)\n",
    "    freq_j = sum(count_j.itervalues())\n",
    "    \n",
    "    for w in vocab:\n",
    "        pi = (1-alpha)*count[w]/float(freq)+alpha*count_ij[w]/float(freq_ij)\n",
    "        pj = (1-alpha)*count_j[w]/float(freq_j)+alpha*count_ij[w]/float(freq_ij)\n",
    "\n",
    "        tmp_Qij += pi * math.log(pj,2)\n",
    "    Q_ij = 0 - tmp_Qij\n",
    "    return Q_ij\n",
    "\n",
    "Q = [[]]*7\n",
    "for i in range(0,7):\n",
    "    Q[i] = [[]]*7\n",
    "\n",
    "for i in range(0,6):   \n",
    "    for j in range(i+1,7):\n",
    "        Q[i][j] = get_Qij(group_vocab[i],group_vocab[j],corpus_vocab[i][j])\n",
    "        Q[j][i] = get_Qij(group_vocab[j],group_vocab[i],corpus_vocab[i][j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate E values between groups\n",
    "E = [[]]*7\n",
    "for i in range(0,7):\n",
    "    E[i] = [[]]*7\n",
    "    \n",
    "for i in range(0,6):   \n",
    "    for j in range(i+1,7):\n",
    "        E[i][j] = H[i] / Q[i][j]\n",
    "        E[j][i] = H[j] / Q[j][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate C values between groups\n",
    "C = [[]]*7\n",
    "E_C = [[]]*7\n",
    "for i in range(0,7):\n",
    "    C[i] = [[]]*7\n",
    "    E_C[i] = [[]]*7\n",
    "for i in range(0,6):   \n",
    "    for j in range(i+1,7):\n",
    "        C[i][j] = 1 - E[i][j]\n",
    "        C[j][i] = 1 - E[j][i]\n",
    "        E_C[i][j] = (C[i][j]+C[j][i])/2\n",
    "        E_C[j][i] = (C[i][j]+C[j][i])/2\n",
    "\n",
    "for i in range(0,7):\n",
    "    E_C[i][i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.603877311999\n",
      "0.354558169854\n",
      "0.232642618479\n",
      "0.222251450889\n",
      "0.218852374037\n",
      "0.243247576346\n",
      "0.389665200183\n"
     ]
    }
   ],
   "source": [
    "# calculate average C values for each group\n",
    "C_avg = [0]*7\n",
    "\n",
    "for i in range(0,7):\n",
    "    for j in range(0,7):\n",
    "        if (i != j):\n",
    "            C_avg[i] += C[i][j]\n",
    "    C_avg[i] = C_avg[i]/6\n",
    "    print C_avg[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
