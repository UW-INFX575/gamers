{
 "metadata": {
  "name": "",
  "signature": "sha256:c9aeb30c0e013b79e08bf03da708481cf7c580bd088467c58dc6d57d8dcec434"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import libraries\n",
      "import re\n",
      "import os\n",
      "from bs4 import BeautifulSoup\n",
      "from urllib2 import urlopen\n",
      "from time import sleep\n",
      "import unicodedata\n",
      "import csv\n",
      "import string\n",
      "\n",
      "def make_soup(url):\n",
      "    html = urlopen(url).read()\n",
      "    return BeautifulSoup(html, \"lxml\")\n",
      "\n",
      "def get_page_links(base_url,i,j):\n",
      "    for i in range(i,j):\n",
      "        page = base_url+'/?page='+str(i)\n",
      "        page_links.append(page)\n",
      "    return True\n",
      "\n",
      "def get_article_links(page_url):\n",
      "    soup = make_soup(page_url)   \n",
      "    links = soup.find_all(\"a\", {\"class\": \"js-event-tracking\"})                \n",
      "    for a in links:\n",
      "        article_links.append(BASE_URL + a['href'])  \n",
      "    return True\n",
      "\n",
      "def get_review_info(article_url):    \n",
      "    soup = make_soup(article_url) \n",
      "    \n",
      "    info = soup.find(\"h3\", {\"class\": \"news-byline\"})\n",
      "    author_name = info.find(\"a\")\n",
      "    author_name = author_name.text\n",
      "    \n",
      "    written_date = info.find(\"time\")\n",
      "    written_date = written_date.text\n",
      "    \n",
      "    review_title = soup.find(\"title\")\n",
      "    review_title = review_title.text\n",
      "    \n",
      "    scores = soup.find_all(\"div\", {\"class\": \"gs-score__cell\"})\n",
      "    for s in scores:\n",
      "        review_score = s.find(\"span\")\n",
      "        if review_score is not None:\n",
      "            review_score = review_score.find(\"span\", {\"itemprop\": \"ratingValue\"})\n",
      "            review_score = review_score.text\n",
      "        else:\n",
      "            review_score = s.text\n",
      "            review_score = review_score.strip()\n",
      "            review_score = review_score.replace('\\n', '')\n",
      "        break\n",
      "        \n",
      "    platforms = soup.find(\"h3\", {\"class\": \"related-game__title\"})\n",
      "    game_platform = platforms.find(\"span\", {\"class\": \"related-game__platforms\"})\n",
      "    game_platform = game_platform.find_all(\"strong\")\n",
      "    g_p = ''\n",
      "    index = 0\n",
      "    for p in game_platform:\n",
      "        game_p = p.text\n",
      "        if (index == 0):           \n",
      "            g_p = g_p + game_p\n",
      "        else:\n",
      "            g_p = g_p + ' & ' + game_p\n",
      "        index += 1\n",
      "    \n",
      "    links = soup.find(\"section\", {\"class\": \"article-body typography-format \"})     \n",
      "    review_text = links.find_all(\"p\") \n",
      "    reviews = []\n",
      "    for p in review_text:\n",
      "        text = p.text        \n",
      "        text = unicodedata.normalize('NFKD', text).encode('ascii','ignore')\n",
      "        reviews.append(text) \n",
      "    return author_name, written_date, review_title, review_score, g_p, reviews\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# set base url for scraping\n",
      "BASE_URL = \"http://www.gamespot.com\"\n",
      "SUB_BASE_URL = \"http://www.gamespot.com/reviews\" \n",
      "\n",
      "# generate links for each page\n",
      "page_links = []\n",
      "\n",
      "# generate following pages\n",
      "# get_page_links(SUB_BASE_URL,1,11)\n",
      "get_page_links(SUB_BASE_URL,13,18)\n",
      "# get_page_links(SUB_BASE_URL,23,28)\n",
      "\n",
      "# generate article links for each page\n",
      "article_links = []\n",
      "for l in page_links:\n",
      "    get_article_links(l)\n",
      "\n",
      "authors = []\n",
      "dates = []\n",
      "titles = []\n",
      "urls = []\n",
      "review_scores = []\n",
      "game_platforms = []\n",
      "review_text = []\n",
      "\n",
      "for link in article_links:\n",
      "    author, date, title, score, platform, review = get_review_info(link)\n",
      "    authors.append(author)\n",
      "    dates.append(date)\n",
      "    titles.append(title)\n",
      "    urls.append(link)\n",
      "    review_scores.append(score)\n",
      "    game_platforms.append(platform)\n",
      "    review_text.append(review)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 107
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "GAMESPOT = authors, dates, titles, urls, review_scores, game_platforms, review_text\n",
      "\n",
      "with open(\"GAMESPOT.csv\", \"wb\") as f:\n",
      "    writer = csv.writer(f)\n",
      "    for row in GAMESPOT:\n",
      "        writer.writerow(row)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 108
    }
   ],
   "metadata": {}
  }
 ]
}