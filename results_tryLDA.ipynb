{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "        \n",
    "reader = csv.reader(open(\"results.csv\", 'rU'), dialect='excel')\n",
    "# reader = pd.read_csv(\"results.csv\", encoding='utf-8')\n",
    "authors = []\n",
    "dates = []\n",
    "titles = []\n",
    "urls = []\n",
    "scores = []\n",
    "platforms = []\n",
    "review_text = []\n",
    "\n",
    "for col in reader:\n",
    "    dates.append(col[0])\n",
    "    titles.append(col[1])\n",
    "    urls.append(col[2])\n",
    "    platforms.append(col[3])\n",
    "    review_text.append(col[4])\n",
    "    authors.append(col[6])\n",
    "    scores.append(col[8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>titles</th>\n",
       "      <th>urls</th>\n",
       "      <th>scores</th>\n",
       "      <th>platforms</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> Jeff Gerstmann</td>\n",
       "      <td> SingStar</td>\n",
       "      <td> http://www.giantbomb.com/api/review/1900-1/</td>\n",
       "      <td> 80</td>\n",
       "      <td> PlayStation 3</td>\n",
       "      <td> All these karaoke video games have got it all ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> Jeff Gerstmann</td>\n",
       "      <td>     Haze</td>\n",
       "      <td> http://www.giantbomb.com/api/review/1900-2/</td>\n",
       "      <td> 40</td>\n",
       "      <td> PlayStation 3</td>\n",
       "      <td> Haze attempts to deal with the concept of wart...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          authors    titles                                         urls  \\\n",
       "0  Jeff Gerstmann  SingStar  http://www.giantbomb.com/api/review/1900-1/   \n",
       "1  Jeff Gerstmann      Haze  http://www.giantbomb.com/api/review/1900-2/   \n",
       "\n",
       "  scores      platforms                                            reviews  \n",
       "0     80  PlayStation 3  All these karaoke video games have got it all ...  \n",
       "1     40  PlayStation 3  Haze attempts to deal with the concept of wart...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "index = range(len(scores))\n",
    "# columns = ['authors','dates','titles','urls','scores','platforms','reviews']\n",
    "columns = ['authors','titles','urls','scores','platforms','reviews']\n",
    "\n",
    "review_data = pd.DataFrame(index=index, columns=columns)\n",
    "review_data['authors'] = authors\n",
    "# review_data['dates'] = dates\n",
    "review_data['titles'] = titles\n",
    "review_data['urls'] = urls\n",
    "review_data['scores'] = scores\n",
    "review_data['platforms'] = platforms\n",
    "review_data['reviews'] = review_text\n",
    "\n",
    "review_data.ix[:1, :7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> Jeff Gerstmann</td>\n",
       "      <td> All these karaoke video games have got it all ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> Jeff Gerstmann</td>\n",
       "      <td> Haze attempts to deal with the concept of wart...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          authors                                            reviews\n",
       "0  Jeff Gerstmann  All these karaoke video games have got it all ...\n",
       "1  Jeff Gerstmann  Haze attempts to deal with the concept of wart..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# index = range(415)\n",
    "columns = ['authors','reviews']\n",
    "\n",
    "author_review = pd.DataFrame(index=index, columns=columns)\n",
    "author_review['authors'] = authors\n",
    "author_review['reviews'] = review_text\n",
    "\n",
    "author_review.ix[:1, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "author_review['reviews'] = author_review[['authors','reviews']].groupby(['authors'])['reviews'].transform(lambda x: ','.join(x))\n",
    "author_review = author_review[['authors','reviews']].drop_duplicates()\n",
    "author_review = author_review.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     index            authors  \\\n",
      "0        0     Jeff Gerstmann   \n",
      "1        2     Brad Shoemaker   \n",
      "2        6         Ryan Davis   \n",
      "3       34    Vinny Caravella   \n",
      "4       87       Andy McCurdy   \n",
      "5      116          Dave Lang   \n",
      "6      201       Drew Scanlon   \n",
      "7      289       Alex Navarro   \n",
      "8      389      Matthew Rorie   \n",
      "9      408     Patrick Klepek   \n",
      "10     630        Dan Ryckert   \n",
      "11     659      Kallie Plagge   \n",
      "12     660      Dan Stapleton   \n",
      "13     661    Miranda Sanchez   \n",
      "14     662     Vince Ingenito   \n",
      "15     664         Mitch Dyer   \n",
      "16     665        Marty Sliva   \n",
      "17     666       Lucy O'Brien   \n",
      "18     667    Meghan Sullivan   \n",
      "19     668       Seth G. Macy   \n",
      "20     669        Luke Reilly   \n",
      "21     670           Cam Shea   \n",
      "22     673        Dave Rudden   \n",
      "23     675     Ryan McCaffrey   \n",
      "24     677      Mikel Reparaz   \n",
      "25     679       Leif Johnson   \n",
      "26     680         Jose Otero   \n",
      "27     681          Rob Zacny   \n",
      "28     685      Mike Mitchell   \n",
      "29     689       Rowan Kaiser   \n",
      "..     ...                ...   \n",
      "103   1437      Britton Peele   \n",
      "104   1438      Jeremy Signor   \n",
      "105   1439     Daniel Starkey   \n",
      "106   1441    Cameron Woolsey   \n",
      "107   1444         Brett Todd   \n",
      "108   1445           Don Saas   \n",
      "109   1446       Jason Venter   \n",
      "110   1447  Miguel Concepcion   \n",
      "111   1448     Nick Capozzoli   \n",
      "112   1449      Austin Walker   \n",
      "113   1450       Kevin VanOrd   \n",
      "114   1457     Justin Haywald   \n",
      "115   1466        Tyler Hicks   \n",
      "116   1472        Matt Cabral   \n",
      "117   1478        Peter Brown   \n",
      "118   1495        Jay Pullman   \n",
      "119   1500     John Robertson   \n",
      "120   1514        Mark Walton   \n",
      "121   1519      Daniel Hindes   \n",
      "122   1522      Danny O'Dwyer   \n",
      "123   1549          Sean Bell   \n",
      "124   1575   Brittany Vincent   \n",
      "125   1615      Chris Watters   \n",
      "126   1632    Randolph Ramsay   \n",
      "127   1652      Shaun McInnis   \n",
      "128   1683      Carolyn Petit   \n",
      "129   1729      Maxwell McGee   \n",
      "130   1793      Joseph Barron   \n",
      "131   1821      Martin Gaston   \n",
      "132   1840     Chris Barylick   \n",
      "\n",
      "                                               reviews  \n",
      "0    All these karaoke video games have got it all ...  \n",
      "1    Metal Gear Solid 4: Guns of the Patriots is an...  \n",
      "2    As much as I appreciated the way 2002�۪s The B...  \n",
      "3    It's no secret that I'm a fan of the Dragon Ba...  \n",
      "4    [EDITOR'S NOTE] When we decided that we wanted...  \n",
      "5    Relic has done a good job distilling the campa...  \n",
      "6    The original                  Dirt did a great...  \n",
      "7    ��Do you like seeing people get hit excruciati...  \n",
      "8    Resident Evil: The Mercenaries 3D--the first h...  \n",
      "9    To simply say Resistance 3 is the best Resista...  \n",
      "10   For better and for worse, Nintendo has never s...  \n",
      "11   ['Because of its name, Ive seen people mistake...  \n",
      "12   ['Even though every mission boils down to basi...  \n",
      "13   ['Erens severe angst, potatoes, horrific titan...  \n",
      "14   [\"Unlike its predecessor, The Witcher 3: Wild ...  \n",
      "15   ['The first thing I did in Life Is Strange -- ...  \n",
      "16   ['Wolfenstein: The Old Blood is a strange beas...  \n",
      "17   ['The act of housekeeping sounds like a curiou...  \n",
      "18   ['Last year I called Final Fantasy X/X-2 HD Re...  \n",
      "19   ['Editors Note: Though Kerbal Space Program ha...  \n",
      "20   ['Project CARS isnt for everybody. Its a serio...  \n",
      "21   ['What better way to have some fun with the tr...  \n",
      "22   [\"Halo has plenty of elements that don't work ...  \n",
      "23   ['When I reviewed State of Decay on Xbox 360 o...  \n",
      "24   [\"As I nudge a slice of bread away from its lo...  \n",
      "25   [\"Console MMORPGs are apparently tricky affair...  \n",
      "26   ['BoxBoy builds a devilishly smart puzzle plat...  \n",
      "27   ['Where StarDrive 1 is a giant, rambling mess ...  \n",
      "28   ['Continuing Sonys 10-year tradition, MLB 15: ...  \n",
      "29   ['Theres just one slight issue with Dungeons 2...  \n",
      "..                                                 ...  \n",
      "103  ['Military skirmishes are never two-dimensiona...  \n",
      "104  [\"At several different points during Wolfenste...  \n",
      "105  [\"It's difficult to wrap your head around how ...  \n",
      "106  [\"Editor's note: Due to a discrepancy between ...  \n",
      "107  [\"Cowled craziness. Thats on tap with Magicka:...  \n",
      "108  [\"At the beginning of Windward, you're present...  \n",
      "109  [\"Color Guardians is a beautiful headache. The...  \n",
      "110  [\"We love playing David to countless video gam...  \n",
      "111  ['A good 4X strategy game is a bit like a slow...  \n",
      "112  ['Project CARS doesnt have any unlockables. Yo...  \n",
      "113  ['Update: Ive now spent time with every versio...  \n",
      "114  [\"BoxBoy! is a game that subverts your expecta...  \n",
      "115  [\"Bullets from cannons and sawblade projectile...  \n",
      "116  [\"Harold is a cleverly crafted, personality-pa...  \n",
      "117  [\"With the reboot in 2011, Mortal Kombat matur...  \n",
      "118  [\"Coming off the heels of a rather sedentary f...  \n",
      "119  ['Just as watching a film at the cinema offers...  \n",
      "120  ['Ah, Mario Party, the game that, on paper at ...  \n",
      "121  ['When I play There Came An Echo, I feel like ...  \n",
      "122  [\"Hotline Miami 2: Wrong Number's action is so...  \n",
      "123  [\"Rejoice, for Capcom has seen fit to release ...  \n",
      "124  [\"Blizzards iconic World of Warcraft has been ...  \n",
      "125  ['For the better part of this new century, a g...  \n",
      "126  [\"The hook for this year's Skylanders entry se...  \n",
      "127  [\"If the two Rock Band sequels were products o...  \n",
      "128  ['What makes life worth living? The Last of Us...  \n",
      "129  ['With Ultra Street Fighter IV, the masterful ...  \n",
      "130  [\"Gran Turismo 6 can be a wonderful thing. It'...  \n",
      "131  ['Welcome to the beautiful city of Los Perdido...  \n",
      "132  ['You can get a lot done with a pink spaghetti...  \n",
      "\n",
      "[133 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print author_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import unicodedata\n",
    "from nltk import stem\n",
    "import string\n",
    "# import sys\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding(\"utf-8\")\n",
    "\n",
    "\n",
    "stopset = set(stopwords.words('english'))\n",
    "letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "special_char = ['~~','``',\"''\", '--',\"'ll\",\"n't\",\"'re\",\"'s\",\"'d\",\"term\"]\n",
    "\n",
    "names=author_review['authors'].unique().tolist()\n",
    "text=author_review['reviews'].unique().tolist()\n",
    "\n",
    "vocab = []\n",
    "\n",
    "for i in range(len(names)):\n",
    "    with open('/Users/feismacbookpro/Desktop/INFX575/Final Project/author_reviews3/'+names[i]+'.txt', 'w') as f:\n",
    "        tokens = word_tokenize(str(text[i]).decode(\"ISO-8859-1\"))\n",
    "        \n",
    "        # remove stop words, stem words, digits, single letters, and special characters\n",
    "        # and get cleaned unigrams\n",
    "        uni_cleaned = ''\n",
    "        vocab_tmp = []\n",
    "        stemmer=stem.PorterStemmer()\n",
    "        for w in tokens:\n",
    "            w = w.lower()\n",
    "            if w not in stopset:\n",
    "                tmp = stemmer.stem(w)\n",
    "                tmp = unicodedata.normalize('NFKD', tmp).encode('ascii','ignore')\n",
    "                if tmp not in string.punctuation:\n",
    "                    if not tmp.isdigit():\n",
    "                        if tmp not in letters:\n",
    "                            if tmp not in special_char:                               \n",
    "                                vocab_tmp.append(tmp)\n",
    "        for u in vocab_tmp:\n",
    "            if (vocab_tmp.count(u) >= 10):\n",
    "                uni_cleaned = uni_cleaned + u + ' '\n",
    "                vocab.append(u)\n",
    "                \n",
    "        f.write(uni_cleaned)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3349\n",
      "3349\n"
     ]
    }
   ],
   "source": [
    "vocab = set(list(vocab))\n",
    "print len(vocab)\n",
    "vocab_new = []\n",
    "# vocab for all 40 combined reviews: vocab_new\n",
    "for v in vocab:\n",
    "    vocab_new.append(v)\n",
    "print len(vocab_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17  0 55 ...,  0  0  0]\n",
      " [ 0  0  0 ...,  0  0  0]\n",
      " [ 0  0  0 ...,  0  0  0]\n",
      " ..., \n",
      " [ 0  0  0 ...,  0  0  0]\n",
      " [ 0  0  0 ...,  0  0  0]\n",
      " [ 0  0  0 ...,  0  0  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/feismacbookpro/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.py:654: DeprecationWarning: The charset_error parameter is deprecated as of version 0.14 and will be removed in 0.16. Use decode_error instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "import os\n",
    "\n",
    "# define a function that creates a term document matrix as pandas dataframe\n",
    "# **kwargs indicates the arguments of CountVectorizer that can be passed\n",
    "def fn_tdm_df(docs, xColNames = None, **kwargs):\n",
    "    #initialize the  vectorizer\n",
    "    vectorizer = CountVectorizer(**kwargs)\n",
    "    x1 = vectorizer.fit_transform(docs)\n",
    "    #create dataFrame\n",
    "    df = pd.DataFrame(x1.toarray().transpose(), index = vectorizer.get_feature_names())\n",
    "    if xColNames is not None:\n",
    "        df.columns = xColNames\n",
    "    return df\n",
    "\n",
    "# define a function that creates corpus from a dictionary\n",
    "def fn_CorpusFromDIR(xDIR):\n",
    "    Res = dict(docs = [open(os.path.join(xDIR,f)).read() for f in os.listdir(xDIR)],\n",
    "               ColNames = map(lambda x: x[0:], os.listdir(xDIR)))\n",
    "    return Res\n",
    " \n",
    "# provide directory where files (to be used to generate the matrix) are saved\n",
    "DIR = '/Users/feismacbookpro/Desktop/INFX575/Final Project/author_reviews2/'\n",
    "\n",
    "# call functions above to create a term document matrix as a pandas dataframe\n",
    "d = fn_tdm_df(docs = fn_CorpusFromDIR(DIR)['docs'],\n",
    "          xColNames = fn_CorpusFromDIR(DIR)['ColNames'], \n",
    "          stop_words='english', charset_error = 'replace') \n",
    "\n",
    "# arrange the result to certain format for LDA use\n",
    "# d = d.drop(['.DS_Store'], axis=1)\n",
    "d_copy = d.transpose()\n",
    "print np.array(d_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 2208)\n"
     ]
    }
   ],
   "source": [
    "print (np.array(d_copy).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero row in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: classic interview theori coolest hill express bourn creatur saint\n",
      "Topic 1: classic theori definit interview separ neverdead voiceov bourn profil\n",
      "Topic 2: classic interview coolest hill tron phantasma massiv express maw\n",
      "Topic 3: classic interview filter bourn hill voiceov theori creatur 'm\n",
      "Topic 4: classic henk bourn monochroma creatur purpos warrior though crimsonland\n",
      "Topic 5: interview neverdead tron theori classic coolest hill filter dedic\n",
      "Topic 6: interview express spread tron bolt filter fist theori card\n",
      "Topic 7: say sunken though dedic develop girl rise scenario mice\n",
      "Topic 8: say soundtrack interview maw first classic hill dedic good\n",
      "Topic 9: classic filter theori human hill interview creatur express bourn\n",
      "Topic 10: interview henk case say hill attempt fault girl scenario\n",
      "Topic 11: neverdead extract rocksmith vessel section loos hardli numer firebal\n",
      "Topic 12: classic read bourn henk hill ive solo one hardlin\n",
      "Topic 13: classic hill coolest tron microphon rise interview case solo\n",
      "Topic 14: classic express coolest voyag siesta dramat foundat impact good\n",
      "Topic 15: classic henk buzz monster \\n\\n paid distract say crimsonland\n",
      "Topic 16: classic hill neverdead scenario filter guild mind gat card\n",
      "Topic 17: theori neverwint hot hill voiceov dialogu express initi girl\n",
      "Topic 18: classic interview hill bourn coolest filter tron voiceov card\n",
      "Topic 19: interview theori coolest dedic hunter hill solo turn-bas bolt\n",
      "Topic 20: classic lich scenario intend soundtrack develop hearth 40k concern\n",
      "Topic 21: interview scenario theori hill coin first filter read hot\n",
      "Topic 22: definit bourn fault shoulder creatur neverdead separ case girl\n",
      "Topic 23: coolest expos rt awesom convinc spelunki guild scenario initi\n",
      "Topic 24: axiom classic creatur interview coolest deathmatch hill theori reach\n",
      "Topic 25: classic hill scenario interview dedic hearth filter bolt express\n",
      "Topic 26: classic garrett interview fist develop syndic monochroma one claptast\n",
      "Topic 27: classic coolest scenario hill filter interview neverdead tron theori\n",
      "Topic 28: classic hill theori interview coolest scenario express filter bourn\n",
      "Topic 29: creatur reach girl 'm deu guild fist saint fault\n",
      "Topic 30: hitman interview hill classic theori express tron filter scenario\n",
      "Topic 31: classic hill coolest interview theori tron express henk yarn\n",
      "Topic 32: kirbi crucial classic interview bullet fallen work wright separ\n",
      "Topic 33: coolest hill theori creatur filter buff tree hearth celceta\n",
      "Topic 34: though doom thorn interview remov shoulder creatur garrett express\n",
      "Topic 35: classic interview express want tron dr. intend dive magnet\n",
      "Topic 36: classic filter hill theori hold coolest interview bolt dedic\n",
      "Topic 37: also duel theori rotat soundtrack psycho nilin hover lumin\n",
      "Topic 38: tree theori girl target wing develop soundtrack mice turn-bas\n",
      "Topic 39: classic coolest interview theori say decid tron henk card\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lda\n",
    "\n",
    "# specify parameters for LDA model -  number of topics, number of iterations, number of words per topic)\n",
    "# and fit the model with generated term document matrix\n",
    "model = lda.LDA(n_topics=40, n_iter=1000, random_state=1)\n",
    "model.fit(np.array(d_copy))\n",
    "\n",
    "topic_word = model.topic_word_\n",
    "n_top_words = 10\n",
    "\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab_new)[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
